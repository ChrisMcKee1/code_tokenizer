# Code Documentation

## Model: gpt-4o

## Statistics

- files_processed: 4
- total_tokens: 2611
- total_size: 12279
- skipped_files: 2
- truncated_files: 0
### Languages
- Python: 2
- Text: 2

- errors: 0

## Processed Files

- __init__.py
- __init__.py,cover
- tokenizer.py
- tokenizer.py,cover

## Failed Files

- __pycache__/__init__.cpython-313.pyc
- __pycache__/tokenizer.cpython-313.pyc
